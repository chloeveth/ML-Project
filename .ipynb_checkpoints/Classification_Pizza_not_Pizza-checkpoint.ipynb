{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4402e16",
   "metadata": {},
   "source": [
    "## Binary Image Classification: Pizza vs. Not Pizza\n",
    "\n",
    "Authors: Chloe Veth and Sharon Dunbar\n",
    "\n",
    "Our goal is to build a binary image classifier. Through applying different machine learning classification techniques, we are hoping to differentiate between images of pizza and images of other food that is not pizza\n",
    "#### Our dataset\n",
    "We found our data on Kaggle: https://www.kaggle.com/datasets/carlosrunner/pizza-not-pizza. The dataset contains 1966 images with an even split between images that are pizza and images that are not. Each image is 512 pixels on the longest side. \n",
    "\n",
    "To download the dataset for yourself, please visit our GitHub repository: https://github.com/chloeveth/ML-Project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b623a8c",
   "metadata": {},
   "source": [
    "#### Libraries that we are using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09204d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for image manipulation\n",
    "import cv2\n",
    "from scipy import ndimage\n",
    "\n",
    "# for displaying graphs and image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for machine learning\n",
    "import sklearn\n",
    "\n",
    "# general purpose \n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68683b55",
   "metadata": {},
   "source": [
    "#### Transforming data\n",
    "These steps will take the training data, transform it, and output the cleaned, ready to use data into a new dataset folder. In the dataset, the images are already resized to have a max length of 512 pixels, but we want images of exactly the same size. The transformation steps include rotating all images to be landscape, changing the aspect ratio of all images to 3:4, and decreasing resolution by half."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7a3aee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "paths = ['/not_pizza/', '/pizza/']\n",
    "\n",
    "for path in paths:\n",
    "    os.makedirs('clean_data' + path, exist_ok=True)\n",
    "    \n",
    "    for filename in os.listdir(\"pizza_not_pizza\" + path):\n",
    "        # load image\n",
    "        img = cv2.imread(\"pizza_not_pizza\" + path + filename)\n",
    "    \n",
    "        # if the image is horizontal, rotate so it's vertical\n",
    "        if img.shape[0] > img.shape[1]:\n",
    "            rot_img = np.transpose(img, (1, 0, 2))  \n",
    "        else:\n",
    "            rot_img = img\n",
    "        \n",
    "        # Semi random decision, change if necessary\n",
    "        # reshape all images to be 3:4 and about half of original dimensions, that is 192 by 256 \n",
    "        # (since all original images have one dim that is 512)\n",
    "        new_img = cv2.resize(rot_img,(256, 192)) # width by height so axes are swapped when passed in\n",
    "\n",
    "        # write image to new directory, preserving dir structure & filenames\n",
    "        new_path = 'clean_data' + path + filename\n",
    "        cv2.imwrite(new_path, new_img)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def801af",
   "metadata": {},
   "source": [
    "#### Loading the data\n",
    "Once the data is transformed and stored to a new folder, we load it into the X array and create targets y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fec08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in data\n",
    "X = []\n",
    "paths = ['/not_pizza/', '/pizza/']\n",
    "for path in paths:\n",
    "    for filename in os.listdir(\"clean_data\" + path):\n",
    "        # load image\n",
    "        img_array = cv2.imread(\"clean_data\" + path + filename)\n",
    "        X.append(img_array.flatten()) # flatten to 1D array\n",
    "    print(f\"files from {path} loaded\")\n",
    "    \n",
    "X = np.array(X)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5e21d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create array of labels, with pizza as class 0 and ice cream as class 1\n",
    "num_not_pizza = len([f for f in os.listdir(\"clean_data/not_pizza\")])\n",
    "num_pizza = len([f for f in os.listdir(\"clean_data/pizza\")])\n",
    "\n",
    "y = np.concatenate((np.zeros(num_not_pizza), np.ones(num_pizza)))\n",
    "\n",
    "# make sure all data is loaded\n",
    "assert len(y) == len(X)\n",
    "assert len(X) == 1966"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e7441b",
   "metadata": {},
   "source": [
    "#### A Sample of the dataset\n",
    "Here is a selection of images from both classes in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98981bb4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "not_pizza_imgs = X[0:15]\n",
    "pizza_imgs = X[983:998]\n",
    "together = np.concatenate((not_pizza_imgs, pizza_imgs))\n",
    "targets_together = np.concatenate((y[0:15], y[983:998]))\n",
    "\n",
    "fig, axes = plt.subplots(6, 5, figsize=(18, 24), subplot_kw={'xticks': (), 'yticks': ()})\n",
    "\n",
    "plt.subplots_adjust(hspace=0)\n",
    "for target, image, ax in zip(targets_together, together, axes.ravel()):\n",
    "    img = np.reshape(image, (192, 256, 3))\n",
    "    ax.imshow(cv2.cvtColor(img, cv2.COLOR_RGBA2BGR))\n",
    "    ax.set_title(\"pizza\" if target == 1 else \"not pizza\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443ae23b",
   "metadata": {},
   "source": [
    "#### Implementing K Nearest Neighbors\n",
    "For starters, we tried the K Nearest Neighbors approach because it was one of the simplest and allowed us to make sure we had loaded the dataset correctly. In order to get an accurate representation of how well it does, we ran it 10 times on different splits of training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d76687",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"Run # \\t Training Score \\t Test Score\")\n",
    "test_total = 0.0\n",
    "training_total = 0.0\n",
    "\n",
    "for x in range(10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "    knn_model = KNeighborsClassifier(n_neighbors=7)\n",
    "    knn_model.fit(X_train, y_train)\n",
    "    training_score = knn_model.score(X_train, y_train)\n",
    "    test_score = knn_model.score(X_test, y_test)\n",
    "    training_total += training_score\n",
    "    test_total += test_score\n",
    "    \n",
    "    print(f\"{x} \\t {training_score} \\t {test_score}\")\n",
    "    \n",
    "print(\"\\nAverage Scores\")\n",
    "print(f\"Training: {training_total / 10}\")\n",
    "print(f\"Test: {test_total / 10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb234f5e",
   "metadata": {},
   "source": [
    "This model does okay, scoring an average of .72 on the training set and .63 on the test set. It potentially doesn't do super well because the dimensions of the datapoints are quite large (147456) so the curse of dimensionality makes neighbors farther apart. There's probably a way we can do this better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f048d03c",
   "metadata": {},
   "source": [
    "#### Trying Principal Component Analysis\n",
    "Hopefully using PCA we can find the most informative components and reduce the dimensionality of the data while still keeping enough information to create a good classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcd2b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "pca = PCA(n_components=200, whiten=True).fit(X_train) # keeps 200 most informative components\n",
    "\n",
    "# data mapped onto pca space\n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e38ab2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying KNN on this transformed data\n",
    "knn_model_pca = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_model_pca.fit(X_train_pca, y_train)\n",
    "knn_model_pca.score(X_test_pca, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60830f1d",
   "metadata": {},
   "source": [
    "The transformed data does a bit worse, getting a score slightly above 50%. With PCA, we reduce the dimensionality and lose some of the accuracy, which probably means that the models are actually using information from all of the dimensions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0656d1",
   "metadata": {},
   "source": [
    "#### Implementing Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d104888",
   "metadata": {},
   "source": [
    "Maybe the problem is that KNN is too simple and a different model might perform better. To explore this possibility, we tried support vector machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e323699",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC()\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "print(\"Accuracy on training set: {:.2f}\".format(svc.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.2f}\".format(svc.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b066335",
   "metadata": {},
   "source": [
    "While the accuracy of this model is higher on both the training and the test set compared to KNN (0.91 vs 0.71 on training and 0.74 vs 0.63 on test), it is also much slower to train. Maybe principal component analysis can help with speed without reducing accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a958990",
   "metadata": {},
   "source": [
    "#### Trying Support Vector Machines with Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc452147",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# all pixels will have same scale\n",
    "# easier to do SVM with PCA because taking too long without PCA\n",
    "svc = SVC()\n",
    "svc.fit(X_train_pca, y_train)\n",
    "\n",
    "print(\"Accuracy on training set: {:.2f}\".format(svc.score(X_train_pca, y_train)))\n",
    "print(\"Accuracy on test set: {:.2f}\".format(svc.score(X_test_pca, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a664b7ef",
   "metadata": {},
   "source": [
    "We can see here that running SVM with PCA gives a pretty similar result to above without PCA, but it is much faster. There is overfitting here; we can see that the accuracy on the training set is much better than the accuracy on the test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98bc580",
   "metadata": {},
   "source": [
    "#### SVM with PCA over 3 trials\n",
    "Next, to see if our results were consistent or just a result of the split between training and test sets, we performed SVM with PCA over 3 trials. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2423efe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "max_iter = 3\n",
    "total_train = 0\n",
    "total_test = 0\n",
    "\n",
    "while (i < max_iter):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    \n",
    "    pca = PCA(n_components=200).fit(X_train) # keeps 200 most informative components\n",
    "    # data mapped onto pca space\n",
    "    X_train_pca = pca.transform(X_train)\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "\n",
    "    svc = SVC()\n",
    "    svc.fit(X_train_pca, y_train)\n",
    "\n",
    "    total_train = total_train + svc.score(X_train_pca, y_train)\n",
    "    total_test = total_test + svc.score(X_test_pca, y_test)\n",
    "    i=i+1\n",
    "    \n",
    "print(\"Accuracy on training set: {:.2f}\".format(total_train/max_iter))\n",
    "print(\"Accuracy on test set: {:.2f}\".format(total_test/max_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cc496a",
   "metadata": {},
   "source": [
    "Running the test over the many trials seems to give about the same results, consistent with the results we got previously. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c780ab",
   "metadata": {},
   "source": [
    "#### Implementing Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9eab5f",
   "metadata": {},
   "source": [
    "Next, we'll see if Logistic Regression does any better than SVM and KNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df30128",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(solver=\"liblinear\").fit(X_train, y_train)\n",
    "\n",
    "print(\"Training set score: {:.3f}\".format(logreg.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(logreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a5fb62",
   "metadata": {},
   "source": [
    "We have a similar issue to SVM where there is overfitting, and here the overfitting is even more extreme. Logistic regression does not work very well. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e6fe69",
   "metadata": {},
   "source": [
    "#### Trying Logistic Regression Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7cc362",
   "metadata": {},
   "source": [
    "Nevertheless, we also implement logistic regression with PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28f0f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(solver=\"liblinear\").fit(X_train_pca, y_train)\n",
    "\n",
    "print(\"Training set score: {:.3f}\".format(logreg.score(X_train_pca, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(logreg.score(X_test_pca, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b370b1d",
   "metadata": {},
   "source": [
    "The results are a bit better for the test set, but worse for the training set. There may be less overfitting, but the results of the classification algorithm are still not very accurate. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497fd11c",
   "metadata": {},
   "source": [
    "#### Implementing Neural Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7744ed20",
   "metadata": {},
   "source": [
    "Now, we'll see if neural nets do any better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c71fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(max_iter=200, alpha=.001, random_state=42, hidden_layer_sizes=([100, ]), solver = \"lbfgs\")\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "print(\"Accuracy on training set: {:.2f}\".format(mlp.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.2f}\".format(mlp.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70516170",
   "metadata": {},
   "source": [
    "It appears a common theme here is overfitting. There is still overfitting here, but the accuracy on the test set is a bit better than logistic regression without PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56155eb1",
   "metadata": {},
   "source": [
    "#### Trying Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a944fc2",
   "metadata": {},
   "source": [
    "The following code tries neural nets with the default setting and PCA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc757ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(random_state=42)\n",
    "mlp.fit(X_train_pca, y_train)\n",
    "\n",
    "print(\"Accuracy on training set: {:.2f}\".format(mlp.score(X_train_pca, y_train)))\n",
    "print(\"Accuracy on test set: {:.2f}\".format(mlp.score(X_test_pca, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cd3153",
   "metadata": {},
   "source": [
    "We get fairly similar results to neural nets without PCA. There is still overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10622f02",
   "metadata": {},
   "source": [
    "In the following code, we adjust some of the settings, mainly changing the alpha to see if that will decrease overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff4e349",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(max_iter=10000, alpha=1, random_state=0, hidden_layer_sizes=([10, ]))\n",
    "mlp.fit(X_train_pca, y_train)\n",
    "\n",
    "print(\"Accuracy on training set: {:.3f}\".format(\n",
    "    mlp.score(X_train_pca, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(mlp.score(X_test_pca, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b61a34",
   "metadata": {},
   "source": [
    "With the adjusted settings, the results are a bit better, but they still are not great. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e57e5a1",
   "metadata": {},
   "source": [
    "#### Using a CNN\n",
    "\n",
    "To do this, we followed the steps for preparing data and building a model that we went through as a class in the lab on Deep learning. This code is copied from that lab and then modified as necessary for our situation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd43387b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cea28d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in data - without flattening\n",
    "X_cnn = []\n",
    "paths = ['/not_pizza/', '/pizza/']\n",
    "for path in paths:\n",
    "    for filename in os.listdir(\"clean_data\" + path):\n",
    "        # load image\n",
    "        img_array = cv2.imread(\"clean_data\" + path + filename)\n",
    "        X_cnn.append(img_array)\n",
    "    print(f\"files from {path} loaded\")\n",
    "    \n",
    "X_cnn = np.array(X_cnn)\n",
    "\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "X_cnn = X_cnn / 255.0\n",
    "print(X_cnn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624c8e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "DefaultConv2D = partial(keras.layers.Conv2D,\n",
    "                        kernel_size=3, activation='relu', padding=\"SAME\")\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    DefaultConv2D(filters=64, kernel_size=7, input_shape=[192, 256, 3]),\n",
    "    keras.layers.MaxPooling2D(pool_size=2),\n",
    "    DefaultConv2D(filters=128),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(units=10, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1c7015",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_cnn, X_test_cnn, y_train_cnn, y_test_cnn = train_test_split(X_cnn, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ac1713",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train_cnn, y_train_cnn, epochs=10)#, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce88f6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test_cnn, y_test_cnn)\n",
    "X_new = X_test_cnn[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87917f8",
   "metadata": {},
   "source": [
    "This simple CNN does relatively well. However, the performance varied greatly based on the various training and test set splits. Because it took around 20 minutes to train, it was difficult to determine what the accuracy averaged to. With 10 epochs, it achieved a score of .99 on the training set and .92 on the test set at one point, but more commonly it would score around .90 on the training set and around .60 on the test set, which is not much of an improvement from some of the simpler models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9099ac0d",
   "metadata": {},
   "source": [
    "#### Conclusions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a31580",
   "metadata": {},
   "source": [
    "In this lab, we were able to try the majority of the classification models that we learned about in class, both with and without the transformation of the data by principal component analysis. Overall, we found that support vector machines without principal component analysis performed the best out of the simpler types of models. Convolutional neural nets also performed well, but maybe not well enough to justify the higher cost of computation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
