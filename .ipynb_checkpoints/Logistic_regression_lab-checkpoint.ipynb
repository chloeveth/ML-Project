{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression\n",
    "\n",
    "The goal of this lab activity is to observe how logistic regression is applied and what the effects of its parameters are. The code snippets are largely adapted from Aurelien Geron, *Hands-On Machine Learning*, pg 139-144 and Muller and Guido, *Introduction to Machine Learning with Python*, pg 58-65. \n",
    "\n",
    "First import the libraries we'll use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision boundaries\n",
    "\n",
    "On Friday we considered what would happen if we used linear regression as a means of classification on the intuition that a regressor's real number result can be mapped to a class. Somehow or another, that mapping needs to be made using a decision boundary: any value below the boundary is class 0, anything above is class 1 (we'll see about extending this to classification to more than two classes later). The naive way to make this decision is to feed the regressor through a step function. \n",
    "\n",
    "As we observed in class, a step function does not have a useful derivative, and so it cannot be used in a loss function for which we can take a gradient. To address this, we use a differentiable approximation of a step function, the *logistic* function $\\sigma(t)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.linspace(-10, 10, 100)\n",
    "sig = 1 / (1 + np.exp(-t))\n",
    "step = np.vectorize(lambda x : 0 if x <=0 else 1)(t)\n",
    "plt.figure(figsize=(9, 3))\n",
    "plt.plot([-10, 10], [0, 0], \"k-\")\n",
    "plt.plot([-10, 10], [0.5, 0.5], \"k:\")\n",
    "plt.plot([-10, 10], [1, 1], \"k:\")\n",
    "plt.plot([0, 0], [-1.1, 1.1], \"k-\")\n",
    "plt.plot(t, sig, \"b-\", linewidth=2, label=r\"$\\sigma(t) = \\frac{1}{1 + e^{-t}}$\")\n",
    "plt.plot(t, step, \"r-\", linewidth=1, label=r\"step(t)\")\n",
    "plt.xlabel(\"t\")\n",
    "plt.legend(loc=\"upper left\", fontsize=20)\n",
    "plt.axis([-10, 10, -0.1, 1.1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic function has several nice properties that make it easy to use for gradients, which we'll explore next time. But for right now, the news is that *logistic regression* is like linear regression but fed into a logistic function to make a smooth decision boundary. Let's see what this model does with data.\n",
    "\n",
    "Let's grab the tried-and-true iris data set, but modify it so that it can be used for binary classification. We'll keep the species Iris Virginica as one class, but toss the other two species (Iris Versicolor and Iris Setosa) together into a second class (non-Virginica)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "# Uncomment the either of the next two lines if you want to review the\n",
    "# details of the iris dataset, including what the features mean\n",
    "#list(iris.keys())\n",
    "#print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's go really simple. Use only a single feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris[\"data\"][:,3].reshape((len(iris[\"data\"]), 1))  # petal width \n",
    "y = (iris[\"target\"] == 2).astype(np.int)  # 1 if Iris-Virginica, else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train a logistic regressor using the implementation from sklearn. The `LogisticRegression` class has a whole bunch of options which we'll examine later. The actual training of the parameters is off-loaded to another library, and you can select which library is used through the `solver` parameter. \"For small datasets, ‘liblinear’ is a good choice,\" according to the sklearn API. Since we're interested in the interaction between the decision boundary and the training data (as opposed to being interested in the accuracy of the classifier), we'll use the *entire dataset* as the training data rather than split it into a training set and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(solver=\"liblinear\")\n",
    "log_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the one feature vs the two classes. (Credit to Geron.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.linspace(0, 3, 1000).reshape(-1, 1)\n",
    "y_proba = log_reg.predict_proba(X_new)\n",
    "decision_boundary = X_new[y_proba[:, 1] >= 0.5][0]\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(X[y==0], y[y==0], \"bs\")\n",
    "plt.plot(X[y==1], y[y==1], \"g^\")\n",
    "plt.plot([decision_boundary, decision_boundary], [-1, 2], \"k:\", linewidth=2)\n",
    "plt.plot(X_new, y_proba[:, 1], \"g-\", linewidth=2, label=\"Iris-Virginica\")\n",
    "plt.plot(X_new, y_proba[:, 0], \"b--\", linewidth=2, label=\"Not Iris-Virginica\")\n",
    "plt.text(decision_boundary+0.02, 0.15, \"Decision  boundary\", fontsize=14, color=\"k\", ha=\"center\")\n",
    "plt.arrow(decision_boundary, 0.08, -0.3, 0, head_width=0.05, head_length=0.1, fc='b', ec='b')\n",
    "plt.arrow(decision_boundary, 0.92, 0.3, 0, head_width=0.05, head_length=0.1, fc='g', ec='g')\n",
    "plt.xlabel(\"Petal width (cm)\", fontsize=14)\n",
    "plt.ylabel(\"Probability\", fontsize=14)\n",
    "plt.legend(loc=\"center left\", fontsize=14)\n",
    "plt.axis([0, 3, -0.02, 1.02])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look carefully in the code above. The method `predict_proba` returns the probability that a given datapoint is in each class. For example, data point 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read that as the probabilities, respectively, of this point being in class 0 and 1. See how the variable `decision_boundary` was computed. This gives the petal width less than which an iris sample is more likely to be Virginica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try the other features, but they don't work as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a feature: 0, 1, or 2 (we already did 3)\n",
    "f = 2\n",
    "\n",
    "X = iris[\"data\"][:,f].reshape((len(X), 1))  \n",
    "y = (iris[\"target\"] == 2).astype(np.int)  # 1 if Iris-Virginica, else 0\n",
    "\n",
    "x_min = min(X[:,0])\n",
    "x_max = max(X[:,0])\n",
    "\n",
    "log_reg = LogisticRegression(solver=\"liblinear\")\n",
    "log_reg.fit(X, y)\n",
    "\n",
    "X_new = np.linspace(x_min, x_max, 1000).reshape(-1, 1)\n",
    "y_proba = log_reg.predict_proba(X_new)\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(X[y==0], y[y==0], \"bs\")\n",
    "plt.plot(X[y==1], y[y==1], \"g^\")\n",
    "plt.plot(X_new, y_proba[:, 1], \"g-\", linewidth=2, label=\"Iris-Virginica\")\n",
    "plt.plot(X_new, y_proba[:, 0], \"b--\", linewidth=2, label=\"Not Iris-Virginica\")\n",
    "plt.xlabel(iris.feature_names[f], fontsize=14)\n",
    "plt.ylabel(\"Probability\", fontsize=14)\n",
    "plt.legend(loc=\"center left\", fontsize=14)\n",
    "plt.axis([x_min, x_max, -0.02, 1.02])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this next plot (also taken from Geron), we look at the decision boundary in two dimensions. Grabbing only petal length and petal width, we train a classifier, plot the points, and show the probability boundaries: (Credit to Geron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
    "y = (iris[\"target\"] == 2).astype(np.int)\n",
    "\n",
    "# C is a parameter controlling regularization, similar to alpha. We'll talk about it more later.\n",
    "log_reg = LogisticRegression(solver=\"liblinear\", C=10**10)\n",
    "log_reg.fit(X, y)\n",
    "\n",
    "x0, x1 = np.meshgrid(\n",
    "        np.linspace(2.9, 7, 500).reshape(-1, 1),\n",
    "        np.linspace(0.8, 2.7, 200).reshape(-1, 1),\n",
    "    )\n",
    "X_new = np.c_[x0.ravel(), x1.ravel()]\n",
    "\n",
    "y_proba = log_reg.predict_proba(X_new)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(X[y==0, 0], X[y==0, 1], \"bs\")\n",
    "plt.plot(X[y==1, 0], X[y==1, 1], \"g^\")\n",
    "\n",
    "zz = y_proba[:, 1].reshape(x0.shape)\n",
    "contour = plt.contour(x0, x1, zz, cmap=plt.cm.brg)\n",
    "\n",
    "\n",
    "left_right = np.array([2.9, 7])\n",
    "boundary = -(log_reg.coef_[0][0] * left_right + log_reg.intercept_[0]) / log_reg.coef_[0][1]\n",
    "\n",
    "plt.clabel(contour, inline=1, fontsize=12)\n",
    "plt.plot(left_right, boundary, \"k--\", linewidth=3)\n",
    "plt.text(3.5, 1.5, \"Not Iris-Virginica\", fontsize=14, color=\"b\", ha=\"center\")\n",
    "plt.text(6.5, 2.3, \"Iris-Virginica\", fontsize=14, color=\"g\", ha=\"center\")\n",
    "plt.xlabel(\"Petal length\", fontsize=14)\n",
    "plt.ylabel(\"Petal width\", fontsize=14)\n",
    "plt.axis([2.9, 7, 0.8, 2.7])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the boundary that the classifier draws is a necessarily a line, but if you look carefully at the points it's apparent that there is no line that perfectly separates them. Logistic regression is linear model.\n",
    "\n",
    "How well does it do as a classifier? (Here we use all four features. Run it several times, since the results you get will be different depending on how the training and test are randomly split.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(iris['data'], (iris[\"target\"] == 2).astype(np.int))\n",
    "log_reg = LogisticRegression(solver=\"liblinear\", C=10**10)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "y_train_classify = log_reg.predict(X_train)\n",
    "y_test_classify = log_reg.predict(X_test)\n",
    "\n",
    "true_virginica = np.sum(np.logical_and(y_test_classify == 1, y_test_classify == y_test))\n",
    "false_virginica = np.sum(np.logical_and(y_test_classify == 1, y_test_classify != y_test))\n",
    "true_non_virginica = np.sum(np.logical_and(y_test_classify == 0, y_test_classify == y_test))\n",
    "false_non_virginica = np.sum(np.logical_and(y_test_classify == 0, y_test_classify != y_test))\n",
    "print(\"Accuracy: \" + str((true_virginica + true_non_virginica)/ len(y_test)))\n",
    "print(\"Precision: \" + str(true_virginica / (true_virginica + false_virginica)))\n",
    "print(\"Recall: \" + str(true_virginica / (true_virginica + false_non_virginica)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression and regularization\n",
    "\n",
    "Think back to the linear regression lab when we visualized the effects of regularization by looking at the values of the parameters for different values of alpha. Here is that plot again, using the California housing data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "calif = fetch_california_housing()\n",
    "X = MinMaxScaler().fit_transform(calif.data)\n",
    "y = calif.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "lr = LinearRegression().fit(X_train, y_train)\n",
    "ridge = Ridge().fit(X_train, y_train)\n",
    "ridge10 = Ridge(alpha=10).fit(X_train, y_train)\n",
    "ridge01 = Ridge(alpha=0.1).fit(X_train, y_train)\n",
    "\n",
    "\n",
    "plt.plot(ridge.coef_, 's', label=\"Ridge alpha=1\")\n",
    "plt.plot(ridge10.coef_, '^', label=\"Ridge alpha=10\")\n",
    "plt.plot(ridge01.coef_, 'v', label=\"Ridge alpha=0.1\")\n",
    "\n",
    "plt.plot(lr.coef_, 'o', label=\"LinearRegression\")\n",
    "plt.xlabel(\"Coefficient index\")\n",
    "plt.ylabel(\"Coefficient value\")\n",
    "xlims = plt.xlim()\n",
    "plt.hlines(0, xlims[0], xlims[1])\n",
    "plt.xlim(xlims)\n",
    "plt.ylim(-25, 25)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For coefficients (that is, parametersss) 2 and 3 (think $\\theta_2$ and $\\theta_3$), the magnitude is large using plain linear regression. Regularization penalizes large large parameters, and you can see the effect of larger alphas pulling those parameters closer to 0.\n",
    "\n",
    "Logistic regression similarly uses regularization. Read what the [documentation for sklearn's LogisticRegression]( https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) says about the parameters `penalty` and `C`. In particular notice that regularization is on by default (using `l2`, so the equivalent of ridge regression) and that `C` is actually the inverse of `alpha` for some reason (greater `C`, less regularization). \n",
    "\n",
    "We're going to test the effects of fiddling with `C`, but first let's load the breast cancer data and see how well it does. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "cancer = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the sklearn's own accuracy scoring function rather than calculate it ourselves. Run the following cell several times to see how much the random train/test splitting makes a difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    cancer.data, cancer.target)\n",
    "logreg = LogisticRegression(solver=\"liblinear\").fit(X_train, y_train)\n",
    "print(\"Training set score: {:.3f}\".format(logreg.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(logreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those results are pretty good and fairly consistent, but you probably observed some trials where the test set scored even higher than the training set. Muller and Guido say that with the training set and test set being that close, we may be underfitting the training data---that is, that there is too much regularization. Remember that by default sklearn's `LogisticRegression` does the equivalent of ridge regression with `C`=1. A greater `C` makes a more flexible model (*flexible* in the sense of more readily fitting the training data), and a lower `C` is more regularized/less flexible/less fitting to the training data. Let's try both directions. (Again, try these several times to observe the effect over several train-test splits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    cancer.data, cancer.target)\n",
    "print(\"C = 1 (default)\")\n",
    "logreg = LogisticRegression(solver=\"liblinear\").fit(X_train, y_train)\n",
    "print(\"Training set score: {:.3f}\".format(logreg.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(logreg.score(X_test, y_test)))\n",
    "print(\"C = 100\")\n",
    "logreg100 = LogisticRegression(solver=\"liblinear\", C=100).fit(X_train, y_train)\n",
    "print(\"Training set score: {:.3f}\".format(logreg100.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(logreg100.score(X_test, y_test)))\n",
    "print(\"C = .01 (default)\")\n",
    "logreg001 = LogisticRegression(solver=\"liblinear\", C=0.01).fit(X_train, y_train)\n",
    "print(\"Training set score: {:.3f}\".format(logreg001.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(logreg001.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you observe?\n",
    "\n",
    "Let's see if we can map this affect over a range of values for `C`. The following plots the accuracy on a training and test set over `C` ranging from .001 to 1000. The `C` values are ranging over (and being plotted on) a log scale. (Again, run this several times.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    cancer.data, cancer.target)\n",
    "c_vals = np.logspace(-3, 4, num=10, base=10)\n",
    "#c_vals = [1.]\n",
    "regs = [LogisticRegression(solver=\"liblinear\", C=c).fit(X_train, y_train) for c in c_vals]\n",
    "train_accs = np.array([lg.score(X_train, y_train) for lg in regs])\n",
    "test_accs = np.array([lg.score(X_test, y_test) for lg in regs])\n",
    "plt.xscale(\"log\")\n",
    "plt.plot(c_vals, train_accs, \"r-\", label=\"Training accuracy\")\n",
    "plt.plot(c_vals, test_accs, \"b-\", label=\"Test accuracy\")\n",
    "plt.legend(loc=\"upper left\", fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with linear and ridge regression, we can observe the effect at the feature level by plotting the coefficient value (that is, weight or theta component) for each feature. (This uses the `logreg`, `logreg100`, and `logreg001` from your most recent executing of the cell before laast. To see how much different splits effect this, re-run that cell.) (Credit to Muller and Guido.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(logreg100.coef_.T, '^', label=\"C=100\")\n",
    "plt.plot(logreg.coef_.T, 'o', label=\"C=1\")\n",
    "plt.plot(logreg001.coef_.T, 'v', label=\"C=0.001\")\n",
    "plt.xticks(range(cancer.data.shape[1]), cancer.feature_names, rotation=90)\n",
    "xlims = plt.xlim()\n",
    "plt.hlines(0, xlims[0], xlims[1])\n",
    "plt.xlim(xlims)\n",
    "plt.ylim(-5, 5)\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Coefficient value\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now go back and redo these recent experiements but with `penalty` set to `\"l1`, that is, to use the equivalent to LASSO regression. See the `LogisticRegression` documentation for more information as necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass classification\n",
    "\n",
    "Finally, how can we use this for classification when we have more than two classes? Since the iris data set has three classes, we'll switch back to that one for this purpose. (The code here is adapted from Geron.) \n",
    "\n",
    "One approach is to take the intuition from our earlier experiement where we singled out Virginica and classified Virginica vs non-Virginica. We can do that repeatedly for each class (Setosa vs non-Setosa, Versicolor vs non-Versicolor) and combine the results to find for each data point a probability that is is in each of the three classes. This is called a \"one vs. rest\" approach, or \"ovr\". The `LogisticRegression` allows us to select this approach through the `multi_class` parameter.\n",
    "\n",
    "(Even though we're using all three classes, we'll use only two features so that we can visualize the results in two dimensions.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
    "y = iris[\"target\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ovr_reg = LogisticRegression(multi_class=\"ovr\",solver=\"liblinear\", C=10)\n",
    "ovr_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can observe decision boundaries (and the regions they bound) and probability gradations on the three classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0, x1 = np.meshgrid(\n",
    "        np.linspace(0, 8, 500).reshape(-1, 1),\n",
    "        np.linspace(0, 3.5, 200).reshape(-1, 1),\n",
    "    )\n",
    "X_new = np.c_[x0.ravel(), x1.ravel()]\n",
    "\n",
    "\n",
    "y_proba = ovr_reg.predict_proba(X_new)\n",
    "y_predict = ovr_reg.predict(X_new)\n",
    "\n",
    "zz1 = y_proba[:, 1].reshape(x0.shape)\n",
    "zz = y_predict.reshape(x0.shape)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(X[y==2, 0], X[y==2, 1], \"g^\", label=\"Iris-Virginica\")\n",
    "plt.plot(X[y==1, 0], X[y==1, 1], \"bs\", label=\"Iris-Versicolor\")\n",
    "plt.plot(X[y==0, 0], X[y==0, 1], \"yo\", label=\"Iris-Setosa\")\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])\n",
    "\n",
    "plt.contourf(x0, x1, zz, cmap=custom_cmap)\n",
    "contour = plt.contour(x0, x1, zz1, cmap=plt.cm.brg)\n",
    "plt.clabel(contour, inline=1, fontsize=12)\n",
    "plt.xlabel(\"Petal length\", fontsize=14)\n",
    "plt.ylabel(\"Petal width\", fontsize=14)\n",
    "plt.legend(loc=\"center left\", fontsize=14)\n",
    "plt.axis([0, 7, 0, 3.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative is to use a function called *softmax*, which we might have time to talk about in class on Wednesday, depending on how long other things take. (If we don't have time to talk about softmax during the logistic regression unit, we'll have another opportunity later when we look at neural nets and related models.) For right now, think of it this way: for a given datapoint, the underlying regressor computes a score for each class, and then the softmax function uses those scores to determine probabilities for that data point being in each class.\n",
    "\n",
    "It turns out that the `liblinear` solver can't do multiclass classification in any other way than one-vs.-rest, so we need to switch to the `lbfgs` solver to use the `multinomial` option, which is another name for softmax in this context. (Again, see the documentation for details on this.) (Credit to Geron for this plot; the previous plot was based on it.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_reg = LogisticRegression(multi_class=\"multinomial\",solver=\"lbfgs\", C=10)\n",
    "softmax_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0, x1 = np.meshgrid(\n",
    "        np.linspace(0, 8, 500).reshape(-1, 1),\n",
    "        np.linspace(0, 3.5, 200).reshape(-1, 1),\n",
    "    )\n",
    "X_new = np.c_[x0.ravel(), x1.ravel()]\n",
    "\n",
    "\n",
    "y_proba = softmax_reg.predict_proba(X_new)\n",
    "y_predict = softmax_reg.predict(X_new)\n",
    "\n",
    "zz1 = y_proba[:, 1].reshape(x0.shape)\n",
    "zz = y_predict.reshape(x0.shape)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(X[y==2, 0], X[y==2, 1], \"g^\", label=\"Iris-Virginica\")\n",
    "plt.plot(X[y==1, 0], X[y==1, 1], \"bs\", label=\"Iris-Versicolor\")\n",
    "plt.plot(X[y==0, 0], X[y==0, 1], \"yo\", label=\"Iris-Setosa\")\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])\n",
    "\n",
    "plt.contourf(x0, x1, zz, cmap=custom_cmap)\n",
    "contour = plt.contour(x0, x1, zz1, cmap=plt.cm.brg)\n",
    "plt.clabel(contour, inline=1, fontsize=12)\n",
    "plt.xlabel(\"Petal length\", fontsize=14)\n",
    "plt.ylabel(\"Petal width\", fontsize=14)\n",
    "plt.legend(loc=\"center left\", fontsize=14)\n",
    "plt.axis([0, 7, 0, 3.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the code cells earlier in this note book, find out how accurate these classifiers are when you split the data into training and test sets. Can you adjust the parameters to the regressor to get better results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further study\n",
    "\n",
    "With the time that remains, experiment with logistic regression on another dataset of your choice. If your semester project involves a classification task, see how well logistic regression works on your data set (or other data sets you have looked at)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
