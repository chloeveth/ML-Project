{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4402e16",
   "metadata": {},
   "source": [
    "## Binary Image Classification: Pizza vs. Ice Cream\n",
    "Our goal is to build a binary image classifier. Through applying different machine learning classification techniques, we are hoping to differentiate between images of pizza and images of other food that is not pizza\n",
    "#### Our dataset\n",
    "We found our data on Kaggle. https://www.kaggle.com/datasets/carlosrunner/pizza-not-pizza The dataset contains 1966 images with an even split between images that are pizza and images that are not. Each image is 512 pixels on the longest side. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b623a8c",
   "metadata": {},
   "source": [
    "#### Libraries that we are using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09204d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for image manipulation\n",
    "import cv2\n",
    "from scipy import ndimage\n",
    "\n",
    "# for displaying graphs and image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for machine learning\n",
    "import sklearn\n",
    "\n",
    "# general purpose \n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68683b55",
   "metadata": {},
   "source": [
    "#### Transforming data\n",
    "These steps will take the training data, transform it, and output the cleaned, ready to use data into a new dataset folder. In the dataset, the images are already resized to have a max length of 512 pixels, but we want images of exactly the same size. The transformation steps include rotating all images to be landscape, changing the aspect ratio of all images to 3:4, and decreasing resolution by half."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7a3aee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "paths = ['/not_pizza/', '/pizza/']\n",
    "\n",
    "for path in paths:\n",
    "    os.makedirs('clean_data' + path, exist_ok=True)\n",
    "    \n",
    "    for filename in os.listdir(\"pizza_not_pizza\" + path):\n",
    "        # load image\n",
    "        img = cv2.imread(\"pizza_not_pizza\" + path + filename)\n",
    "    \n",
    "        # if the image is horizontal, rotate so it's vertical\n",
    "        if img.shape[0] > img.shape[1]:\n",
    "            rot_img = np.transpose(img, (1, 0, 2))  \n",
    "        else:\n",
    "            rot_img = img\n",
    "        \n",
    "        # Semi random decision, change if necessary\n",
    "        # reshape all images to be 3:4 and about half of original dimensions, that is 192 by 256 \n",
    "        # (since all original images have one dim that is 512)\n",
    "        new_img = cv2.resize(rot_img,(256, 192)) # width by height so axes are swapped when passed in\n",
    "\n",
    "        # write image to new directory, preserving dir structure & filenames\n",
    "        new_path = 'clean_data' + path + filename\n",
    "        cv2.imwrite(new_path, new_img)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def801af",
   "metadata": {},
   "source": [
    "#### Loading the data\n",
    "Once the data is transformed and stored to a new folder, we load it into the X array and create targets y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00fec08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files from /not_pizza/ loaded\n",
      "files from /pizza/ loaded\n",
      "(1966, 147456)\n"
     ]
    }
   ],
   "source": [
    "# load in data\n",
    "X = []\n",
    "paths = ['/not_pizza/', '/pizza/']\n",
    "for path in paths:\n",
    "    for filename in os.listdir(\"clean_data\" + path):\n",
    "        # load image\n",
    "        img_array = cv2.imread(\"clean_data\" + path + filename)\n",
    "        X.append(img_array.flatten()) # flatten to 1D array\n",
    "    print(f\"files from {path} loaded\")\n",
    "    \n",
    "X = np.array(X)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e5e21d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create array of labels, with pizza as class 0 and ice cream as class 1\n",
    "num_not_pizza = len([f for f in os.listdir(\"clean_data/not_pizza\")])\n",
    "num_pizza = len([f for f in os.listdir(\"clean_data/pizza\")])\n",
    "\n",
    "y = np.concatenate((np.zeros(num_not_pizza), np.ones(num_pizza)))\n",
    "\n",
    "# make sure all data is loaded\n",
    "assert len(y) == len(X)\n",
    "assert len(X) == 1966"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e7441b",
   "metadata": {},
   "source": [
    "#### A Sample of the dataset\n",
    "Here is a selection of images from both classes in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98981bb4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "not_pizza_imgs = X[0:15]\n",
    "pizza_imgs = X[983:998]\n",
    "together = np.concatenate((not_pizza_imgs, pizza_imgs))\n",
    "targets_together = np.concatenate((y[0:15], y[983:998]))\n",
    "\n",
    "fig, axes = plt.subplots(6, 5, figsize=(18, 24), subplot_kw={'xticks': (), 'yticks': ()})\n",
    "\n",
    "plt.subplots_adjust(hspace=0)\n",
    "for target, image, ax in zip(targets_together, together, axes.ravel()):\n",
    "    img = np.reshape(image, (192, 256, 3))\n",
    "    ax.imshow(cv2.cvtColor(img, cv2.COLOR_RGBA2BGR))\n",
    "    ax.set_title(\"pizza\" if target == 1 else \"not pizza\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443ae23b",
   "metadata": {},
   "source": [
    "#### Implementing K Nearest Neighbors\n",
    "For starters, we tried the K Nearest Neighbors approach because it was one of the simplest and allowed us to make sure we had loaded the dataset correctly. In order to get an accurate representation of how well it does, we ran it 10 times on different splits of training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d76687",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"Run # \\t Training Score \\t Test Score\")\n",
    "test_total = 0.0\n",
    "training_total = 0.0\n",
    "\n",
    "for x in range(10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "    knn_model = KNeighborsClassifier(n_neighbors=7)\n",
    "    knn_model.fit(X_train, y_train)\n",
    "    training_score = knn_model.score(X_train, y_train)\n",
    "    test_score = knn_model.score(X_test, y_test)\n",
    "    training_total += training_score\n",
    "    test_total += test_score\n",
    "    \n",
    "    print(f\"{x} \\t {training_score} \\t {test_score}\")\n",
    "    \n",
    "print(\"\\nAverage Scores\")\n",
    "print(f\"Training: {training_total / 10}\")\n",
    "print(f\"Test: {test_total / 10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb234f5e",
   "metadata": {},
   "source": [
    "This model does ok, scoring an average of .72 on the training set and .63 on the test set. It potentially doesn't do super well because the dimensions of the datapoints are quite large (147456) so the curse of dimensionality makes neighboers farther apart. There's probably a way we can do this better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f048d03c",
   "metadata": {},
   "source": [
    "#### Trying Principal Component Analysis\n",
    "Hopefully using PCA we can find the most informative components and reduce the dimensionality of the data while still keeping enough information to create a good classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcd2b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "pca = PCA(n_components=200, whiten=True).fit(X_train) # keeps 200 most informative components\n",
    "\n",
    "# data mapped onto pca space\n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e38ab2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying KNN on this transformed data\n",
    "knn_model_pca = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_model_pca.fit(X_train_pca, y_train)\n",
    "knn_model_pca.score(X_test_pca, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0656d1",
   "metadata": {},
   "source": [
    "#### Implementing Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d104888",
   "metadata": {},
   "source": [
    "Maybe the problem is that KNN is too simple and a different model might perform better. To explore this possibility, we tried support vector machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e323699",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC()\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "print(\"Accuracy on training set: {:.2f}\".format(svc.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.2f}\".format(svc.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a958990",
   "metadata": {},
   "source": [
    "#### Trying Support Vector Machines with Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc452147",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# all pixels will have same scale\n",
    "# easier to do SVM with PCA because taking too long without PCA\n",
    "svc = SVC()\n",
    "svc.fit(X_train_pca, y_train)\n",
    "\n",
    "print(\"Accuracy on training set: {:.2f}\".format(svc.score(X_train_pca, y_train)))\n",
    "print(\"Accuracy on test set: {:.2f}\".format(svc.score(X_test_pca, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602ae6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SVM with PCA over many trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2423efe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "max_iter = 3\n",
    "total_train = 0\n",
    "total_test = 0\n",
    "\n",
    "while (i < max_iter):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    \n",
    "    pca = PCA(n_components=200).fit(X_train) # keeps 200 most informative components\n",
    "    # data mapped onto pca space\n",
    "    X_train_pca = pca.transform(X_train)\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "\n",
    "    svc = SVC()\n",
    "    svc.fit(X_train_pca, y_train)\n",
    "\n",
    "    total_train = total_train + svc.score(X_train_pca, y_train)\n",
    "    total_test = total_test + svc.score(X_test_pca, y_test)\n",
    "    i=i+1\n",
    "    \n",
    "print(\"Accuracy on training set: {:.2f}\".format(total_train/max_iter))\n",
    "print(\"Accuracy on test set: {:.2f}\".format(total_test/max_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c780ab",
   "metadata": {},
   "source": [
    "#### Implementing Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df30128",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(solver=\"liblinear\").fit(X_train, y_train)\n",
    "\n",
    "print(\"Training set score: {:.3f}\".format(logreg.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(logreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e6fe69",
   "metadata": {},
   "source": [
    "#### Trying Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28f0f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(solver=\"liblinear\").fit(X_train_pca, y_train)\n",
    "\n",
    "print(\"Training set score: {:.3f}\".format(logreg.score(X_train_pca, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(logreg.score(X_test_pca, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497fd11c",
   "metadata": {},
   "source": [
    "#### Implementing Neural Nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c71fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(max_iter=200, alpha=.001, random_state=42, hidden_layer_sizes=([100, ]), solver = \"lbfgs\")\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "print(\"Accuracy on training set: {:.2f}\".format(mlp.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.2f}\".format(mlp.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56155eb1",
   "metadata": {},
   "source": [
    "#### Trying Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a944fc2",
   "metadata": {},
   "source": [
    "The following code tries neural nets with the defaul setting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc757ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(random_state=42)\n",
    "mlp.fit(X_train_pca, y_train)\n",
    "\n",
    "print(\"Accuracy on training set: {:.2f}\".format(mlp.score(X_train_pca, y_train)))\n",
    "print(\"Accuracy on test set: {:.2f}\".format(mlp.score(X_test_pca, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10622f02",
   "metadata": {},
   "source": [
    "In the following code, we can adjust some of the settings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff4e349",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(max_iter=10000, alpha=1, random_state=0, hidden_layer_sizes=([10, ]))\n",
    "mlp.fit(X_train_pca, y_train)\n",
    "\n",
    "print(\"Accuracy on training set: {:.3f}\".format(\n",
    "    mlp.score(X_train_pca, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(mlp.score(X_test_pca, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e57e5a1",
   "metadata": {},
   "source": [
    "#### Using a CNN\n",
    "\n",
    "To do this, we followed the steps for preparing data and building a model that we went through as a class in the lab on Deep learning. This code is copied from that lab and then modified as necessary for our situation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd43387b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cea28d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files from /not_pizza/ loaded\n",
      "files from /pizza/ loaded\n",
      "(1966, 192, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "# load in data - without flattening\n",
    "X_cnn = []\n",
    "paths = ['/not_pizza/', '/pizza/']\n",
    "for path in paths:\n",
    "    for filename in os.listdir(\"clean_data\" + path):\n",
    "        # load image\n",
    "        img_array = cv2.imread(\"clean_data\" + path + filename)\n",
    "        X_cnn.append(img_array)\n",
    "    print(f\"files from {path} loaded\")\n",
    "    \n",
    "X_cnn = np.array(X_cnn)\n",
    "\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "X_cnn = X_cnn / 255.0\n",
    "print(X_cnn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "624c8e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "DefaultConv2D = partial(keras.layers.Conv2D,\n",
    "                        kernel_size=3, activation='relu', padding=\"SAME\")\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    DefaultConv2D(filters=64, kernel_size=7, input_shape=[192, 256, 3]),\n",
    "    keras.layers.MaxPooling2D(pool_size=2),\n",
    "    DefaultConv2D(filters=128),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(units=10, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f1c7015",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_cnn, X_test_cnn, y_train_cnn, y_test_cnn = train_test_split(X_cnn, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93ac1713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "50/50 [==============================] - 258s 5s/step - loss: 0.4865 - accuracy: 0.7793\n",
      "Epoch 2/10\n",
      "50/50 [==============================] - 229s 5s/step - loss: 0.2660 - accuracy: 0.8963\n",
      "Epoch 3/10\n",
      "50/50 [==============================] - 231s 5s/step - loss: 0.1270 - accuracy: 0.9536\n",
      "Epoch 4/10\n",
      "50/50 [==============================] - 230s 5s/step - loss: 0.2475 - accuracy: 0.9179\n",
      "Epoch 5/10\n",
      "50/50 [==============================] - 235s 5s/step - loss: 0.1589 - accuracy: 0.9529\n",
      "Epoch 6/10\n",
      "50/50 [==============================] - 230s 5s/step - loss: 0.0933 - accuracy: 0.9739\n",
      "Epoch 7/10\n",
      "50/50 [==============================] - 228s 5s/step - loss: 0.0382 - accuracy: 0.9911\n",
      "Epoch 8/10\n",
      "50/50 [==============================] - 228s 5s/step - loss: 0.0564 - accuracy: 0.9924\n",
      "Epoch 9/10\n",
      "50/50 [==============================] - 228s 5s/step - loss: 0.0541 - accuracy: 0.9911\n",
      "Epoch 10/10\n",
      "50/50 [==============================] - 228s 5s/step - loss: 0.0124 - accuracy: 0.9987\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train_cnn, y_train_cnn, epochs=10)#, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce88f6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 17s 1s/step - loss: 0.6151 - accuracy: 0.9264\n",
      "1/1 [==============================] - 0s 492ms/step\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test)\n",
    "X_new = X_test[:10]\n",
    "y_pred = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87917f8",
   "metadata": {},
   "source": [
    "This simple CNN does really well! It took around 20 minutes to train, but with 10 epochs, it achieved a score of .99 on the training set and .92 on the test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9099ac0d",
   "metadata": {},
   "source": [
    "#### Conclusions\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
