{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Activity: Support Vector Machines\n",
    "\n",
    "The goal of this lab activity is to demonstrate the use of support vector machines for classification using the tools available in `scikit-learn`.\n",
    "\n",
    "This activity is adapted from M&uuml;ller and Guido, *Introduction to Machine Learning with Python*, pg 58-68 and 94-105, along with certain excerpts from Geron, *Hands-On Machine Learning*, pg 147-159.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import mglearn  # Generates a deprecation warning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear models for classification\n",
    "\n",
    "The general context for using support vector machines is classifying data using linear formulas that discriminate among classes. As we saw in class on Monday, we separate data points in different classes using a hyperplane defined by\n",
    "\n",
    "$$\n",
    "\\mathbf{w}\\cdot \\mathbf{x} + b = 0\n",
    "$$\n",
    "\n",
    "...with points classified by whether they give a positive or negative value for $\\mathbf{w}\\cdot \\mathbf{x} + b$. \n",
    "There are several approaches to classification based on hyperplanes, not just SVMs. \n",
    "They differ from each other on how to find an appropriate hyperplane based on the data, how to tolerate\n",
    "(or otherwise deal with) misclassifications, etc.\n",
    "\n",
    "One point of comparison for support vector machines is logistic regression, \n",
    "since in essence they are both about finding lines/hyperplanes that separate data.\n",
    "Consider this example of applying them to a training data set using their `scikit-learn` implementation with default parameters. (This generates a couple of warnings, but you can ignore them.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
    "y = iris[\"target\"]\n",
    "setosa_or_versicolor = (y == 0) | (y == 1)\n",
    "X = X[setosa_or_versicolor]\n",
    "y = y[setosa_or_versicolor]\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 3))\n",
    "\n",
    "for model, ax in zip([LinearSVC(), LogisticRegression(solver=\"liblinear\")], axes):\n",
    "    clf = model.fit(X, y)\n",
    "    mglearn.plots.plot_2d_separator(clf, X, fill=False, eps=0.5,\n",
    "                                    ax=ax, alpha=.7)\n",
    "    mglearn.discrete_scatter(X[:, 0], X[:, 1], y, ax=ax)\n",
    "    ax.set_title(clf.__class__.__name__)\n",
    "    ax.set_xlabel(\"Feature 0\")\n",
    "    ax.set_ylabel(\"Feature 1\")\n",
    "axes[0].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `C` in `LinearSVC` is for *classifier*. The data is not linearly separable, and although the lines drawn by `LinearSVC` and `LogisticRegression` aren't too far off, you can see that they do draw different lines.\n",
    "(I would like to have the actual support vectors appear in the SVM plot, but `LinearSVC` doesn't store the\n",
    "support vectors themselves, just the hyperplane.)\n",
    "\n",
    "In class last time we talked about how a soft margin classification is parameterized by `C`\n",
    "(which the `scikit-learn` documentation calls the *penalty parameter*). \n",
    "Consider the effect on the classification by these various settings of that parameter,\n",
    "used on artificial data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from mglearn.plot_helpers import discrete_scatter\n",
    "import numpy as np\n",
    "\n",
    "X, y = make_blobs(centers=2, random_state=4, n_samples=30)\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "# a carefully hand-designed dataset \n",
    "y[7] = 0\n",
    "y[27] = 0\n",
    "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "\n",
    "for ax, C in zip(axes, [1,10,1e3]): #[1e-2, 10, 1e3]):\n",
    "    discrete_scatter(X[:, 0], X[:, 1], y, ax=ax)\n",
    "\n",
    "    svm = LinearSVC(C=C, tol=0.00001, dual=False).fit(X, y)\n",
    "    w = svm.coef_[0]\n",
    "    a = -w[0] / w[1]\n",
    "    xx = np.linspace(6, 13)\n",
    "    yy = a * xx - (svm.intercept_[0]) / w[1]\n",
    "    ax.plot(xx, yy, c='k')\n",
    "    ax.set_xlim(x_min, x_max)\n",
    "    ax.set_ylim(y_min, y_max)\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    ax.set_title(\"C = %f\" % C)\n",
    "axes[0].legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you observe? \n",
    "How many points are misclassified, and how far off are they?\n",
    "Which displays evidence of overfitting?\n",
    "\n",
    "Now try again on real data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
    "y = iris[\"target\"]\n",
    "versicolor_or_virginica = (y == 1) | (y == 2)\n",
    "X = X[versicolor_or_virginica]\n",
    "y = y[versicolor_or_virginica]\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "\n",
    "for ax, C in zip(axes, [1, 10, 1e3]):\n",
    "    discrete_scatter(X[:, 0], X[:, 1], y, ax=ax)\n",
    "\n",
    "    svm = LinearSVC(C=C, tol=0.00001, dual=False).fit(X, y)\n",
    "    w = svm.coef_[0]\n",
    "    a = -w[0] / w[1]\n",
    "    xx = np.linspace(x_min, x_max)\n",
    "    yy = a * xx - (svm.intercept_[0]) / w[1]\n",
    "    ax.plot(xx, yy, c='k')\n",
    "    ax.set_xlim(x_min, x_max)\n",
    "    ax.set_ylim(y_min, y_max)\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    ax.set_title(\"C = %f\" % C)\n",
    "axes[0].legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multiclass classification\n",
    "\n",
    "Our discussion in class assumed binary classification, and in fact a straight-up\n",
    "linear approach inherently applies only to two classes.\n",
    "There are still ways to extend the idea to more than two classes.\n",
    "One way is to break the problem up into several binary classification problems, one\n",
    "for each class. \n",
    "The idea is that each of several classifiers can distinguish one class from all the others,\n",
    "called *one-vs-rest*.\n",
    "For a new data point, each classifier tests that point and identifies it as either in or out of\n",
    "the class. \n",
    "\n",
    "Ideally, exactly one classifier accepts that new point.\n",
    "If no classifier identifies the point as being in-class, then the point will have to be tagged\n",
    "as whatever class it was closest to.\n",
    "If more than one classifier accepts the point, then the tie will have to be broken\n",
    "by some measurement of how confident the classifiers are.\n",
    "\n",
    "Here we try this out with three classes (artificial data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs(random_state=42)\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")\n",
    "plt.legend([\"Class 0\", \"Class 1\", \"Class 2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These classes are linearly separable, so they should be easy to classify. \n",
    "Let's fit an SVM classifier to them. A `LinearSVC` has an array of coefficients \n",
    "($\\mathbf{w}$, since they are analogous to what we call *weights* in other contexts)\n",
    "and an intercept ($b$, analogous to the *bias*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_svm = LinearSVC().fit(X, y)\n",
    "print(str(linear_svm.coef_))\n",
    "print(str(linear_svm.intercept_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you understand what these numbers mean and why the arrays have the shape that they do.\n",
    "Now we plot the lines with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "line = np.linspace(-15, 15)\n",
    "for coef, intercept, color in zip(linear_svm.coef_, linear_svm.intercept_,\n",
    "                                  mglearn.cm3.colors):\n",
    "    plt.plot(line, -(line * coef[0] + intercept) / coef[1], c=color)\n",
    "plt.ylim(-10, 15)\n",
    "plt.xlim(-10, 8)\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")\n",
    "plt.legend(['Class 0', 'Class 1', 'Class 2', 'Line class 0', 'Line class 1',\n",
    "            'Line class 2'], loc=(1.01, 0.3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have three us-vs-them lines. \n",
    "There are three regions where there is no ambiguity about how a new data point would be classified,\n",
    "and the training data are all nicely in an appropriate region.\n",
    "There are also three overlap regions, and a \"no man's land\" in the middle. \n",
    "We can visualize how the decisions would go in these cases by shading the \n",
    "plot by class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.plots.plot_2d_classification(linear_svm, X, fill=True, alpha=.7)\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "line = np.linspace(-15, 15)\n",
    "for coef, intercept, color in zip(linear_svm.coef_, linear_svm.intercept_,\n",
    "                                  mglearn.cm3.colors):\n",
    "    plt.plot(line, -(line * coef[0] + intercept) / coef[1], c=color)\n",
    "plt.legend(['Class 0', 'Class 1', 'Class 2', 'Line class 0', 'Line class 1',\n",
    "            'Line class 2'], loc=(1.01, 0.3))\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The protocol for disambiguating points that land in overlap regions\n",
    "or no-man's land is straightforward: find the closest boundary line.\n",
    "How well does this perform?\n",
    "Go back to the beginning of this section and revise the code so that it generates a\n",
    "larger set of data (the `n_samples` parameter to `make_blobs`).\n",
    "Split that into training and test sets, and test the accuracy.\n",
    "\n",
    "(Remember that there are library functions that make this easy such as\n",
    "`sklearn.model_selection.train_test_split`.\n",
    "Like other classifiers, `LinearSVC` has a `score` method to compute accuracy on\n",
    "a test set.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SVMs and non-linearly-separable data\n",
    "\n",
    "The SVM classifiers we've looked at so far are called *linear* in scikit learn because they are not kernelized, and `LinearSVC` does not directly support using kernels.\n",
    "If the data isn't even close to being linearly separable, then there is little one can do \n",
    "using only linear models.\n",
    "Consider this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(centers=4, random_state=8)\n",
    "y = y % 2\n",
    "\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try fitting a linear SVM to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "linear_svm = LinearSVC().fit(X, y)\n",
    "\n",
    "mglearn.plots.plot_2d_separator(linear_svm, X)\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In class we talked about adding dimensionality to a dataset to make the data separable.\n",
    "Of course this data is separable, just not *linearly* separable.\n",
    "It turns out that in this example, squaring \"feature 1\" sharply discriminates the blues\n",
    "(high absolute value for feature 1) from the the oranges (low absolute value for feature 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the squared first feature\n",
    "X_new = np.hstack([X, X[:, 1:] ** 2])\n",
    "\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D, axes3d\n",
    "figure = plt.figure()\n",
    "# visualize in 3D\n",
    "ax = Axes3D(figure, elev=-152, azim=-26)\n",
    "# plot first all the points with y==0, then all with y == 1\n",
    "mask = y == 0\n",
    "ax.scatter(X_new[mask, 0], X_new[mask, 1], X_new[mask, 2], c='b',\n",
    "           cmap=mglearn.cm2, s=60, edgecolor='k')\n",
    "ax.scatter(X_new[~mask, 0], X_new[~mask, 1], X_new[~mask, 2], c='r', marker='^',\n",
    "           cmap=mglearn.cm2, s=60, edgecolor='k')\n",
    "ax.set_xlabel(\"feature0\")\n",
    "ax.set_ylabel(\"feature1\")\n",
    "ax.set_zlabel(\"feature1 ** 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can make a plane that slices the data, putting the orange on one side and the blue on the other. \n",
    "Here we train an SVM on the new features. Note that this is still using `LinearSVC`, because we transformed the data ourselves and are now applying a linear SVM to that transformed data---that's different from using an SVM classifier that itself supports kernelization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_svm_3d = LinearSVC().fit(X_new, y)\n",
    "coef, intercept = linear_svm_3d.coef_.ravel(), linear_svm_3d.intercept_\n",
    "\n",
    "# show linear decision boundary\n",
    "figure = plt.figure()\n",
    "ax = Axes3D(figure, elev=-152, azim=-26)\n",
    "xx = np.linspace(X_new[:, 0].min() - 2, X_new[:, 0].max() + 2, 50)\n",
    "yy = np.linspace(X_new[:, 1].min() - 2, X_new[:, 1].max() + 2, 50)\n",
    "\n",
    "XX, YY = np.meshgrid(xx, yy)\n",
    "ZZ = (coef[0] * XX + coef[1] * YY + intercept) / -coef[2]\n",
    "ax.plot_surface(XX, YY, ZZ, rstride=8, cstride=8, alpha=0.3)\n",
    "ax.scatter(X_new[mask, 0], X_new[mask, 1], X_new[mask, 2], c='b',\n",
    "           cmap=mglearn.cm2, s=60, edgecolor='k')\n",
    "ax.scatter(X_new[~mask, 0], X_new[~mask, 1], X_new[~mask, 2], c='r', marker='^',\n",
    "           cmap=mglearn.cm2, s=60, edgecolor='k')\n",
    "\n",
    "ax.set_xlabel(\"feature0\")\n",
    "ax.set_ylabel(\"feature1\")\n",
    "ax.set_zlabel(\"feature1 ** 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can take that classifier and apply it to the original data set.\n",
    "We can see how the plane is projected on the original feature space by coloring the background. \n",
    "This shows us the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZZ = YY ** 2\n",
    "dec = linear_svm_3d.decision_function(np.c_[XX.ravel(), YY.ravel(), ZZ.ravel()])\n",
    "plt.contourf(XX, YY, dec.reshape(XX.shape), levels=[dec.min(), 0, dec.max()],\n",
    "             cmap=mglearn.cm2, alpha=0.5)\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(This next part is adapted from Geron, *Hands-On Machine Learning*, pg 152-154).\n",
    "\n",
    "Let's try another artifically-generated and artificially-bad dataset, this time by the `make_moons` function, so called because the data look like two half moons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=100, noise=0.15, random_state=42)\n",
    "\n",
    "plt.axis([-1.5, 2.5, -1, 1.5])\n",
    "plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"bs\")\n",
    "plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"g^\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a plain SVM is not going to give good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = LinearSVC(C=C, tol=0.00001, dual=False).fit(X, y)\n",
    "w = svm.coef_[0]\n",
    "a = -w[0] / w[1]\n",
    "xx = np.linspace(-1.5, 2.5)\n",
    "yy = a * xx - (svm.intercept_[0]) / w[1]\n",
    "plt.plot(xx, yy, c='k')\n",
    "plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"bs\")\n",
    "plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"g^\")\n",
    "plt.axis([-1.5, 2.5, -1, 1.5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_moons_svm_boundary(X, y, d, r, C) :\n",
    "    svm = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"classifier\", SVC(kernel = \"poly\", degree=d, coef0=r, C=C))\n",
    "    ])\n",
    "    svm.fit(X, y)\n",
    "    x0s = np.linspace(-1.5, 2.5, 100)\n",
    "    x1s = np.linspace(-1, 1.5, 100)\n",
    "    x0, x1 = np.meshgrid(x0s, x1s)\n",
    "    X_pred = np.c_[x0.ravel(), x1.ravel()]\n",
    "    y_pred = svm.predict(X_pred).reshape(x0.shape)\n",
    "    y_decision = svm.decision_function(X_pred).reshape(x0.shape)\n",
    "    plt.contourf(x0, x1, y_pred, cmap=plt.cm.brg, alpha=0.2)\n",
    "    plt.contourf(x0, x1, y_decision, cmap=plt.cm.brg, alpha=0.1)\n",
    "    plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"bs\")\n",
    "    plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"g^\")\n",
    "    plt.title(\"d=\" + str(d) + \", r=\" + str(r) + \", C=\" + str(C), fontsize=18)\n",
    "    plt.axis([-1.5, 2.5, -1, 1.5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play around with the parameters: *d* is the degree of the polynomial features, *r* (or `coef0`) is how much the model is influenced by high-degree polynomial features vs low-degree, and *C* is how soft the magin is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_moons_svm_boundary(X, y, 10, 100, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. SVMs and the data\n",
    "\n",
    "Now let's try this on some real data.\n",
    "The following code will load the breast cancer data set and train an SVM using \n",
    "the default parameters (which you can look up in \n",
    "[the documentation](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) ).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "cancer = sklearn.datasets.load_breast_cancer()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    cancer.data, cancer.target, random_state=0)\n",
    "\n",
    "svc = SVC()\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "print(\"Accuracy on training set: {:.2f}\".format(svc.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.2f}\".format(svc.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a pretty low score on the test set.\n",
    "The high score on the training set looks like overfitting.\n",
    "Can you do better by adjusting the parameters?\n",
    "\n",
    "Of course in real life we don't want to fiddle with the parameters blindly.\n",
    "Let's see if looking at the data tells us something.\n",
    "The following code makes a \"min-max\" plot of the features in this data set.\n",
    "Each bar shows the range of values for a particular feature. \n",
    "Note that the vertical axis is on a log scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(X_train, manage_ticks=False)\n",
    "plt.yscale(\"symlog\")\n",
    "plt.xlabel(\"Feature index\")\n",
    "plt.ylabel(\"Feature magnitude\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What this shows us is that different features are on very different scales, and some\n",
    "have large ranges, others small. \n",
    "The differences are in orders of magnitude.\n",
    "For good performance, SVMs need the features to be similarly scaled.\n",
    "\n",
    "We can help by preprocessing (rescaling) the data. \n",
    "Let's rescale each feature so that the values range from 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the minimum value per feature on the training set\n",
    "min_on_training = X_train.min(axis=0)\n",
    "# Compute the range of each feature (max - min) on the training set\n",
    "range_on_training = (X_train - min_on_training).max(axis=0)\n",
    "\n",
    "# subtract the min, divide by range\n",
    "# afterward, min=0 and max=1 for each feature\n",
    "X_train_scaled = (X_train - min_on_training) / range_on_training\n",
    "\n",
    "# Sanity check\n",
    "print(\"Minimum for each feature\\n\", X_train_scaled.min(axis=0))\n",
    "print(\"Maximum for each feature\\n\", X_train_scaled.max(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use THE SAME transformation on the test set,\n",
    "# using min and range of the training set. See Chapter 3 (unsupervised learning) for details.\n",
    "X_test_scaled = (X_test - min_on_training) / range_on_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how that does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC()\n",
    "svc.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Accuracy on training set: {:.3f}\".format(\n",
    "        svc.score(X_train_scaled, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(svc.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that's a lot better. But now we aren't getting the training set perfectly. \n",
    "Let's increase `C`. Remember, larger `C` means fewer misclassifications on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(C=1000)\n",
    "svc.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Accuracy on training set: {:.3f}\".format(\n",
    "    svc.score(X_train_scaled, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(svc.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tada. \n",
    "\n",
    "Now try this on your data set for your term project, if applicable.\n",
    "\n",
    "If time permits, or your project data isn't appropriate, then try this on other data sets (digits, iris, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
